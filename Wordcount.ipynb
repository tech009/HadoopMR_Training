{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello World for Hadoop "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Count training on Hadoop 3.3.0 with the dataset containing one of the Shakespeare Play ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        text = line.strip()\n",
    "    except ValueError as e:\n",
    "        continue\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        print(\"{}\\t{}\".format(word.lower(), str(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "\n",
    "import sys\n",
    "\n",
    "curword = None\n",
    "count = 0\n",
    "word = None\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    word,value = line.split('\\t',1)\n",
    "    if word == curword:\n",
    "        count += int(value)\n",
    "    else:\n",
    "        if curword:\n",
    "            print('{}\\t{}'.format(curword,str(count)))\n",
    "        curword = word\n",
    "        count = int(value)\n",
    "if curword == word:\n",
    "    print('{}\\t{}'.format(curword,str(count)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted WordCountTask\n",
      "packageJobJar: [/tmp/hadoop-unjar2813490275726333320/] [] /tmp/streamjob7940354349421265021.jar tmpDir=null\n",
      "\"'tis\t1\n",
      "\"a\t4\n",
      "\"as-is\".\t1\n",
      "\"defect\"\t1\n",
      "\"i\t4\n",
      "\"king\t1\n",
      "\"lo,\t1\n",
      "\"not\"\t1\n",
      "\"project\t1\n",
      "\"sing\t2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-16 21:00:46,014 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-10-16 21:00:46,259 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-10-16 21:00:46,463 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/isuser/.staging/job_1602668066400_0054\n",
      "2020-10-16 21:00:46,805 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2020-10-16 21:00:46,872 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "2020-10-16 21:00:47,033 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1602668066400_0054\n",
      "2020-10-16 21:00:47,034 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2020-10-16 21:00:47,251 INFO conf.Configuration: resource-types.xml not found\n",
      "2020-10-16 21:00:47,252 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2020-10-16 21:00:47,313 INFO impl.YarnClientImpl: Submitted application application_1602668066400_0054\n",
      "2020-10-16 21:00:47,350 INFO mapreduce.Job: The url to track the job: http://localhost:8088/proxy/application_1602668066400_0054/\n",
      "2020-10-16 21:00:47,352 INFO mapreduce.Job: Running job: job_1602668066400_0054\n",
      "2020-10-16 21:00:53,478 INFO mapreduce.Job: Job job_1602668066400_0054 running in uber mode : false\n",
      "2020-10-16 21:00:53,480 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2020-10-16 21:01:00,666 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "2020-10-16 21:01:01,673 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2020-10-16 21:01:06,721 INFO mapreduce.Job:  map 100% reduce 13%\n",
      "2020-10-16 21:01:09,816 INFO mapreduce.Job:  map 100% reduce 25%\n",
      "2020-10-16 21:01:11,839 INFO mapreduce.Job:  map 100% reduce 38%\n",
      "2020-10-16 21:01:14,880 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "2020-10-16 21:01:15,890 INFO mapreduce.Job:  map 100% reduce 88%\n",
      "2020-10-16 21:01:16,893 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2020-10-16 21:01:16,901 INFO mapreduce.Job: Job job_1602668066400_0054 completed successfully\n",
      "2020-10-16 21:01:17,019 INFO mapreduce.Job: Counters: 55\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=989763\n",
      "\t\tFILE: Number of bytes written=4661350\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=5462479\n",
      "\t\tHDFS: Number of bytes written=640196\n",
      "\t\tHDFS: Number of read operations=46\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=16\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=8\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=10528\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=50661\n",
      "\t\tTotal time spent by all map tasks (ms)=10528\n",
      "\t\tTotal time spent by all reduce tasks (ms)=50661\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=10528\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=50661\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=10780672\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=51876864\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=124456\n",
      "\t\tMap output records=901325\n",
      "\t\tMap output bytes=6743784\n",
      "\t\tMap output materialized bytes=989811\n",
      "\t\tInput split bytes=184\n",
      "\t\tCombine input records=901325\n",
      "\t\tCombine output records=79061\n",
      "\t\tReduce input groups=59508\n",
      "\t\tReduce shuffle bytes=989811\n",
      "\t\tReduce input records=79061\n",
      "\t\tReduce output records=59508\n",
      "\t\tSpilled Records=158122\n",
      "\t\tShuffled Maps =16\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=16\n",
      "\t\tGC time elapsed (ms)=1706\n",
      "\t\tCPU time spent (ms)=16370\n",
      "\t\tPhysical memory (bytes) snapshot=2612957184\n",
      "\t\tVirtual memory (bytes) snapshot=28019154944\n",
      "\t\tTotal committed heap usage (bytes)=2012217344\n",
      "\t\tPeak Map Physical memory (bytes)=359243776\n",
      "\t\tPeak Map Virtual memory (bytes)=2796044288\n",
      "\t\tPeak Reduce Physical memory (bytes)=257884160\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2805706752\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=5462295\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=640196\n",
      "2020-10-16 21:01:17,020 INFO streaming.StreamJob: Output directory: WordCountTask\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "OUT_DIR=\"WordCountTask\"\n",
    "NUM_REDUCERS=8\n",
    "\n",
    "hdfs dfs -rm -r -skipTrash ${OUT_DIR}\n",
    "\n",
    "yarn jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.3.0.jar \\\n",
    "    -D mapreduce.job.name=\"Streaming Word Count\" \\\n",
    "    -D mapreduce.job.reduces=${NUM_REDUCERS} \\\n",
    "    -files mapper.py,reducer.py \\\n",
    "    -mapper \"python mapper.py\" \\\n",
    "    -combiner \"python reducer.py\" \\\n",
    "    -reducer \"python reducer.py\" \\\n",
    "    -input /dataset/data.txt \\\n",
    "    -output ${OUT_DIR}\n",
    "\n",
    "hdfs dfs -cat ${OUT_DIR}/part-00000 | head"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
